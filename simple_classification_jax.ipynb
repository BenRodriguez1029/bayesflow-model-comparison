{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpagkHbkVrEmoFRw/FdvzP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenRodriguez1029/bayesflow-model-comparison/blob/main/simple_classification_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sflnhyn5KLQd",
        "outputId": "ecd0d729-ab41-4fa7-a481-3a959771f254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jax\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "import keras\n",
        "from keras.utils import Sequence\n",
        "from keras.layers import Layer, Dense, GlobalAveragePooling1D, Reshape\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.models import Sequential\n",
        "import keras.ops as ops\n",
        "\n",
        "import numpy as np\n",
        "import bayesflow as bf\n",
        "import random\n",
        "\n",
        "from losses import logistic_loss, exponential_loss, alpha_log_exponential_loss\n",
        "\n",
        "print(keras.backend.backend())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simulator and Data Preparation"
      ],
      "metadata": {
        "id": "7oKPDLgtgsOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create model simulators\n",
        "\n",
        "def prior_alternative():\n",
        "    return np.random.normal(loc=0, scale=1)\n",
        "\n",
        "def sample_model_0(sample_size, n=30):\n",
        "    samples = np.random.normal(loc=0, scale=1, size=(sample_size, n))\n",
        "    return samples\n",
        "\n",
        "def sample_model_1(sample_size, n=30):\n",
        "    mus = np.array([prior_alternative() for _ in range(sample_size)])\n",
        "\n",
        "    samples = np.random.normal(loc=mus[:, None], scale=1, size=(sample_size, n))\n",
        "    return samples"
      ],
      "metadata": {
        "id": "e4kzLeeoK0Ox"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Sequence):\n",
        "    def __init__(self, X, y, batch_size=32, shuffle=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.indices = np.arange(len(X))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.X) + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indices = self.indices[index*self.batch_size : (index+1)*self.batch_size]\n",
        "        X_batch = self.X[batch_indices]\n",
        "        y_batch = self.y[batch_indices]\n",
        "        return X_batch, y_batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)"
      ],
      "metadata": {
        "id": "cU9N7ErWK6AW"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simulate data\n",
        "sample_size = 2000\n",
        "\n",
        "data_0 = sample_model_0(sample_size)\n",
        "data_1 = sample_model_1(sample_size)\n",
        "\n",
        "split = int(0.8 * data_0.shape[0])\n",
        "data_0_train, data_0_val = data_0[:split], data_0[split:]\n",
        "data_1_train, data_1_val = data_1[:split], data_1[split:]\n",
        "\n",
        "data_train = np.concatenate([data_0_train, data_1_train], axis=0)\n",
        "data_val = np.concatenate([data_0_val, data_1_val], axis=0)\n",
        "\n",
        "label_train = np.concatenate([np.zeros(data_0_train.shape[0]), np.ones(data_1_train.shape[0])], axis=0)\n",
        "label_val = np.concatenate([np.zeros(data_0_val.shape[0]), np.ones(data_1_val.shape[0])], axis=0)\n",
        "\n",
        "perm = np.random.permutation(len(data_train))\n",
        "data_train = data_train[perm]\n",
        "label_train = label_train[perm]\n",
        "\n",
        "# dataloader\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = MyDataset(data_train[:, None, :], label_train[:, None], batch_size=32)\n",
        "val_loader   = MyDataset(data_val[:, None, :], label_val[:, None], batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "OFZsiIFvLA0q"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network"
      ],
      "metadata": {
        "id": "rzNtgKqZgxIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EvidenceNetwork(keras.Model):\n",
        "    def __init__(self, output, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # shared backbone network\n",
        "        self.summary_network = bf.networks.DeepSet(summary_dim=8, dropout=None)\n",
        "        self.classification_network = bf.networks.MLP(widths=[32] * 4, activation=\"silu\", dropout=None)\n",
        "\n",
        "        # output layer depends on output type\n",
        "        if output == \"p\":\n",
        "            self.output_layer = Dense(1, activation=\"sigmoid\",\n",
        "                                      kernel_initializer=RandomNormal(mean=0.0, stddev=0.01))  # probability 0-1\n",
        "        elif output == \"K\":\n",
        "            self.output_layer = Dense(1,activation=\"softplus\",\n",
        "                                      kernel_initializer=RandomNormal(mean=np.log(np.exp(1)-1), stddev=0.01))  # strictly positive\n",
        "        elif output == \"log(K)\":\n",
        "            self.output_layer = Dense(1, activation=None,\n",
        "                                       kernel_initializer=RandomNormal(mean=0.0, stddev=0.01))  # unbounded\n",
        "        else:\n",
        "            raise ValueError(\"Invalid output type\")\n",
        "\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.summary_network(inputs, training=training)\n",
        "        x = self.classification_network(x, training=training)\n",
        "        return self.output_layer(x)"
      ],
      "metadata": {
        "id": "c7o2UZQ_LI-O"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bayes_accuracy(y_true, f_x):\n",
        "    p = ops.sigmoid(f_x)\n",
        "    preds = ops.cast(p > 0.5, \"float32\")\n",
        "    return ops.mean(ops.cast(ops.equal(preds, ops.cast(y_true, \"float32\")), \"float32\"))\n",
        "\n",
        "def alpha_log_exponential_accuracy(y_true, y_pred, alpha=2.0):\n",
        "    y_true = ops.cast(y_true, dtype='float32')\n",
        "    pred_labels = ops.cast(ops.greater(y_pred, 0.5), dtype='float32')\n",
        "    correct = ops.equal(pred_labels, y_true)\n",
        "    return ops.mean(ops.cast(correct, dtype='float32'), axis=-1)\n",
        "\n",
        "classifier_log = EvidenceNetwork(\"log(K)\")\n",
        "classifier_log.compile(optimizer=\"adam\", loss=logistic_loss, metrics=[bayes_accuracy])\n",
        "\n",
        "classifier_ce = EvidenceNetwork(\"p\")\n",
        "classifier_ce.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "classifier_exp = EvidenceNetwork(\"log(K)\")\n",
        "classifier_exp.compile(optimizer=\"adam\", loss=exponential_loss, metrics=[bayes_accuracy])\n",
        "\n",
        "classifier_alpha_exp_log = EvidenceNetwork(\"K\")\n",
        "classifier_alpha_exp_log.compile(optimizer=\"adam\", loss=alpha_log_exponential_loss, metrics=[alpha_log_exponential_accuracy])"
      ],
      "metadata": {
        "id": "B6bxiLN5g1x6"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "9EnOTbXliqIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "epochs = 20\n",
        "\n",
        "history_log = classifier_log.fit(\n",
        "    train_loader,\n",
        "    validation_data=val_loader,\n",
        "    epochs=epochs,\n",
        ")\n",
        "\n",
        "history_ce = classifier_ce.fit(\n",
        "    train_loader,\n",
        "    validation_data=val_loader,\n",
        "    epochs=epochs,\n",
        ")\n",
        "\n",
        "history_exp = classifier_exp.fit(\n",
        "    train_loader,\n",
        "    validation_data=val_loader,\n",
        "    epochs=epochs,\n",
        ")\n",
        "\n",
        "history_alpha_exp_log = classifier_alpha_exp_log.fit(\n",
        "    train_loader,\n",
        "    validation_data=val_loader,\n",
        "    epochs=epochs,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV_EDolFLSjk",
        "outputId": "5d73c55e-348e-4e6d-fa6b-6b2593cb5308"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 89ms/step - bayes_accuracy: 0.6733 - loss: 0.6017 - val_bayes_accuracy: 0.7950 - val_loss: 0.4960\n",
            "Epoch 2/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - bayes_accuracy: 0.8005 - loss: 0.4433 - val_bayes_accuracy: 0.7862 - val_loss: 0.4843\n",
            "Epoch 3/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.8288 - loss: 0.3962 - val_bayes_accuracy: 0.7912 - val_loss: 0.4406\n",
            "Epoch 4/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - bayes_accuracy: 0.8366 - loss: 0.3834 - val_bayes_accuracy: 0.8037 - val_loss: 0.4386\n",
            "Epoch 5/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.8521 - loss: 0.3576 - val_bayes_accuracy: 0.7950 - val_loss: 0.4515\n",
            "Epoch 6/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - bayes_accuracy: 0.8562 - loss: 0.3480 - val_bayes_accuracy: 0.7962 - val_loss: 0.4919\n",
            "Epoch 7/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.8694 - loss: 0.3273 - val_bayes_accuracy: 0.8025 - val_loss: 0.4861\n",
            "Epoch 8/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.8697 - loss: 0.3269 - val_bayes_accuracy: 0.7937 - val_loss: 0.4921\n",
            "Epoch 9/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.8832 - loss: 0.3056 - val_bayes_accuracy: 0.7675 - val_loss: 0.6533\n",
            "Epoch 10/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - bayes_accuracy: 0.8909 - loss: 0.3011 - val_bayes_accuracy: 0.7788 - val_loss: 0.5089\n",
            "Epoch 11/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.8926 - loss: 0.2887 - val_bayes_accuracy: 0.7987 - val_loss: 0.4559\n",
            "Epoch 12/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - bayes_accuracy: 0.8996 - loss: 0.2828 - val_bayes_accuracy: 0.8000 - val_loss: 0.4871\n",
            "Epoch 13/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - bayes_accuracy: 0.9077 - loss: 0.2653 - val_bayes_accuracy: 0.8075 - val_loss: 0.5593\n",
            "Epoch 14/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - bayes_accuracy: 0.9129 - loss: 0.2415 - val_bayes_accuracy: 0.7937 - val_loss: 0.4679\n",
            "Epoch 15/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - bayes_accuracy: 0.8826 - loss: 0.3087 - val_bayes_accuracy: 0.7800 - val_loss: 0.5129\n",
            "Epoch 16/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.9184 - loss: 0.2270 - val_bayes_accuracy: 0.7837 - val_loss: 0.5789\n",
            "Epoch 17/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.9139 - loss: 0.2331 - val_bayes_accuracy: 0.7825 - val_loss: 0.7197\n",
            "Epoch 18/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - bayes_accuracy: 0.9306 - loss: 0.2058 - val_bayes_accuracy: 0.7788 - val_loss: 0.6983\n",
            "Epoch 19/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - bayes_accuracy: 0.9264 - loss: 0.2204 - val_bayes_accuracy: 0.7850 - val_loss: 0.5735\n",
            "Epoch 20/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.9367 - loss: 0.1984 - val_bayes_accuracy: 0.7650 - val_loss: 0.5581\n",
            "Epoch 1/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 93ms/step - accuracy: 0.5747 - loss: 0.6911 - val_accuracy: 0.5800 - val_loss: 0.6708\n",
            "Epoch 2/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6362 - loss: 0.6417 - val_accuracy: 0.7437 - val_loss: 0.5282\n",
            "Epoch 3/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7746 - loss: 0.5140 - val_accuracy: 0.7387 - val_loss: 0.5596\n",
            "Epoch 4/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8203 - loss: 0.4386 - val_accuracy: 0.7950 - val_loss: 0.4429\n",
            "Epoch 5/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8286 - loss: 0.3991 - val_accuracy: 0.7825 - val_loss: 0.4741\n",
            "Epoch 6/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8322 - loss: 0.3972 - val_accuracy: 0.7875 - val_loss: 0.4614\n",
            "Epoch 7/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8568 - loss: 0.3578 - val_accuracy: 0.7975 - val_loss: 0.4560\n",
            "Epoch 8/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8529 - loss: 0.3492 - val_accuracy: 0.7875 - val_loss: 0.4744\n",
            "Epoch 9/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8633 - loss: 0.3396 - val_accuracy: 0.7887 - val_loss: 0.4813\n",
            "Epoch 10/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8565 - loss: 0.3367 - val_accuracy: 0.7788 - val_loss: 0.5277\n",
            "Epoch 11/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8802 - loss: 0.3186 - val_accuracy: 0.7837 - val_loss: 0.5365\n",
            "Epoch 12/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8834 - loss: 0.2972 - val_accuracy: 0.7825 - val_loss: 0.4982\n",
            "Epoch 13/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8929 - loss: 0.2837 - val_accuracy: 0.7713 - val_loss: 0.5730\n",
            "Epoch 14/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8961 - loss: 0.2844 - val_accuracy: 0.7725 - val_loss: 0.6589\n",
            "Epoch 15/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8915 - loss: 0.2886 - val_accuracy: 0.7738 - val_loss: 0.5104\n",
            "Epoch 16/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8938 - loss: 0.2666 - val_accuracy: 0.7750 - val_loss: 0.5300\n",
            "Epoch 17/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8895 - loss: 0.3011 - val_accuracy: 0.7700 - val_loss: 0.5350\n",
            "Epoch 18/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8960 - loss: 0.2764 - val_accuracy: 0.7800 - val_loss: 0.6069\n",
            "Epoch 19/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9088 - loss: 0.2682 - val_accuracy: 0.7700 - val_loss: 0.5960\n",
            "Epoch 20/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9282 - loss: 0.2178 - val_accuracy: 0.7812 - val_loss: 0.5551\n",
            "Epoch 1/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 98ms/step - bayes_accuracy: 0.6165 - loss: 0.9637 - val_bayes_accuracy: 0.7812 - val_loss: 0.7483\n",
            "Epoch 2/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - bayes_accuracy: 0.7966 - loss: 0.7117 - val_bayes_accuracy: 0.7688 - val_loss: 0.7601\n",
            "Epoch 3/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.8005 - loss: 0.7285 - val_bayes_accuracy: 0.7850 - val_loss: 0.7404\n",
            "Epoch 4/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.7939 - loss: 0.7568 - val_bayes_accuracy: 0.7600 - val_loss: 0.7536\n",
            "Epoch 5/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.7533 - loss: 0.8272 - val_bayes_accuracy: 0.7688 - val_loss: 0.8154\n",
            "Epoch 6/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.7850 - loss: 0.7817 - val_bayes_accuracy: 0.7875 - val_loss: 0.8200\n",
            "Epoch 7/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - bayes_accuracy: 0.7895 - loss: 0.7288 - val_bayes_accuracy: 0.7887 - val_loss: 0.7928\n",
            "Epoch 8/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - bayes_accuracy: 0.8262 - loss: 0.6956 - val_bayes_accuracy: 0.7912 - val_loss: 0.8334\n",
            "Epoch 9/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - bayes_accuracy: 0.7861 - loss: 0.7653 - val_bayes_accuracy: 0.7188 - val_loss: 0.8527\n",
            "Epoch 10/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - bayes_accuracy: 0.7974 - loss: 0.6874 - val_bayes_accuracy: 0.7887 - val_loss: 0.8137\n",
            "Epoch 11/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.8153 - loss: 0.6336 - val_bayes_accuracy: 0.7987 - val_loss: 4.1241\n",
            "Epoch 12/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.8392 - loss: 0.6022 - val_bayes_accuracy: 0.7750 - val_loss: 1.3171\n",
            "Epoch 13/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.8140 - loss: 0.6203 - val_bayes_accuracy: 0.7725 - val_loss: 1.6019\n",
            "Epoch 14/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.7609 - loss: 0.7441 - val_bayes_accuracy: 0.6475 - val_loss: 3.1328\n",
            "Epoch 15/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.6124 - loss: 1.8184 - val_bayes_accuracy: 0.7525 - val_loss: 0.8299\n",
            "Epoch 16/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.7164 - loss: 0.8282 - val_bayes_accuracy: 0.6275 - val_loss: 1.0953\n",
            "Epoch 17/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.6731 - loss: 0.8394 - val_bayes_accuracy: 0.7600 - val_loss: 1.6738\n",
            "Epoch 18/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.7475 - loss: 0.7630 - val_bayes_accuracy: 0.7862 - val_loss: 1.2814\n",
            "Epoch 19/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - bayes_accuracy: 0.8021 - loss: 0.6770 - val_bayes_accuracy: 0.7812 - val_loss: 0.8642\n",
            "Epoch 20/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - bayes_accuracy: 0.8238 - loss: 0.6321 - val_bayes_accuracy: 0.7212 - val_loss: 0.8558\n",
            "Epoch 1/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 79ms/step - alpha_log_exponential_accuracy: 0.5092 - loss: 2.0303 - val_alpha_log_exponential_accuracy: 0.5000 - val_loss: 0.0000e+00\n",
            "Epoch 2/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - alpha_log_exponential_accuracy: 0.4929 - loss: 0.0000e+00 - val_alpha_log_exponential_accuracy: 0.5000 - val_loss: 0.0000e+00\n",
            "Epoch 3/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - alpha_log_exponential_accuracy: 0.4927 - loss: 0.0000e+00 - val_alpha_log_exponential_accuracy: 0.5000 - val_loss: 0.0000e+00\n",
            "Epoch 4/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - alpha_log_exponential_accuracy: 0.4885 - loss: 0.0000e+00 - val_alpha_log_exponential_accuracy: 0.5000 - val_loss: 0.0000e+00\n",
            "Epoch 5/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - alpha_log_exponential_accuracy: 0.5010 - loss: 0.0000e+00 - val_alpha_log_exponential_accuracy: 0.5000 - val_loss: 0.0000e+00\n",
            "Epoch 6/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - alpha_log_exponential_accuracy: 0.4854 - loss: 0.0000e+00 - val_alpha_log_exponential_accuracy: 0.5000 - val_loss: 0.0000e+00\n",
            "Epoch 7/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - alpha_log_exponential_accuracy: 0.5102 - loss: 0.0000e+00 - val_alpha_log_exponential_accuracy: 0.5000 - val_loss: 0.0000e+00\n",
            "Epoch 8/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - alpha_log_exponential_accuracy: 0.4966 - loss: 0.0000e+00 - val_alpha_log_exponential_accuracy: 0.5000 - val_loss: 0.0000e+00\n",
            "Epoch 9/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - alpha_log_exponential_accuracy: 0.4818 - loss: 0.0000e+00 - val_alpha_log_exponential_accuracy: 0.5000 - val_loss: 0.0000e+00\n",
            "Epoch 10/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - alpha_log_exponential_accuracy: 0.5056 - loss: 0.0000e+00 - val_alpha_log_exponential_accuracy: 0.5000 - val_loss: 0.0000e+00\n",
            "Epoch 11/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - alpha_log_exponential_accuracy: 0.5003 - loss: 0.0000e+00 - val_alpha_log_exponential_accuracy: 0.5000 - val_loss: 0.0000e+00\n",
            "Epoch 12/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - alpha_log_exponential_accuracy: 0.4965 - loss: 0.0000e+00 - val_alpha_log_exponential_accuracy: 0.5000 - val_loss: 0.0000e+00\n",
            "Epoch 13/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - alpha_log_exponential_accuracy: 0.4973 - loss: 0.0000e+00 - val_alpha_log_exponential_accuracy: 0.5000 - val_loss: 0.0000e+00\n",
            "Epoch 14/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - alpha_log_exponential_accuracy: 0.4981 - loss: 0.0000e+00 - val_alpha_log_exponential_accuracy: 0.5000 - val_loss: 0.0000e+00\n",
            "Epoch 15/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - alpha_log_exponential_accuracy: 0.5065 - loss: 0.0000e+00 - val_alpha_log_exponential_accuracy: 0.5000 - val_loss: 0.0000e+00\n",
            "Epoch 16/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - alpha_log_exponential_accuracy: 0.5175 - loss: 0.0000e+00 - val_alpha_log_exponential_accuracy: 0.5000 - val_loss: 0.0000e+00\n",
            "Epoch 17/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - alpha_log_exponential_accuracy: 0.4994 - loss: 0.0000e+00 - val_alpha_log_exponential_accuracy: 0.5000 - val_loss: 0.0000e+00\n",
            "Epoch 18/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - alpha_log_exponential_accuracy: 0.4951 - loss: 0.0000e+00 - val_alpha_log_exponential_accuracy: 0.5000 - val_loss: 0.0000e+00\n",
            "Epoch 19/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - alpha_log_exponential_accuracy: 0.5115 - loss: 0.0000e+00 - val_alpha_log_exponential_accuracy: 0.5000 - val_loss: 0.0000e+00\n",
            "Epoch 20/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - alpha_log_exponential_accuracy: 0.5061 - loss: 0.0000e+00 - val_alpha_log_exponential_accuracy: 0.5000 - val_loss: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "-MRm9Cpcg7Ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model_per_class_by_output(model, output_type=\"p\", sample_size=100, n=30):\n",
        "    if output_type == \"p\":\n",
        "        def get_label(y_pred):\n",
        "            if isinstance(y_pred, tuple):\n",
        "                y_pred = y_pred[0]\n",
        "            return int(tf.cast(y_pred > 0.5, tf.int32).numpy()[0,0])\n",
        "    elif output_type == \"K\":\n",
        "        def get_label(y_pred):\n",
        "            if isinstance(y_pred, tuple):\n",
        "                y_pred = y_pred[0]\n",
        "            return int(tf.cast(y_pred > 1.0, tf.int32).numpy()[0,0])\n",
        "    elif output_type == \"log(K)\":\n",
        "        def get_label(y_pred):\n",
        "            if isinstance(y_pred, tuple):\n",
        "                y_pred = y_pred[0]\n",
        "            return int(tf.cast(y_pred > 0.0, tf.int32).numpy()[0,0])\n",
        "    else:\n",
        "        raise ValueError(\"Invalid output_type. Must be 'p', 'K', or 'log(K)'.\")\n",
        "\n",
        "    correct_0 = 0\n",
        "    total_0 = 0\n",
        "    correct_1 = 0\n",
        "    total_1 = 0\n",
        "\n",
        "    for i in range(sample_size):\n",
        "        if i < sample_size // 2:\n",
        "            x = sample_model_0(1)\n",
        "            y_true = 0\n",
        "        else:\n",
        "            x = sample_model_1(1)\n",
        "            y_true = 1\n",
        "\n",
        "        x_input = x[None, :, :]\n",
        "        x_input_tf = tf.convert_to_tensor(x_input, dtype=tf.float32)\n",
        "\n",
        "        y_pred = model(x_input_tf)\n",
        "        pred_label = get_label(y_pred)\n",
        "\n",
        "        if y_true == 0:\n",
        "            total_0 += 1\n",
        "            if pred_label == 0:\n",
        "                correct_0 += 1\n",
        "        else:\n",
        "            total_1 += 1\n",
        "            if pred_label == 1:\n",
        "                correct_1 += 1\n",
        "\n",
        "    acc_0 = correct_0 / total_0\n",
        "    acc_1 = correct_1 / total_1\n",
        "\n",
        "    print(f\"Model 0 accuracy: {acc_0*100:.2f}% ({correct_0}/{total_0})\")\n",
        "    print(f\"Model 1 accuracy: {acc_1*100:.2f}% ({correct_1}/{total_1})\")\n",
        "\n",
        "    return acc_0, acc_1"
      ],
      "metadata": {
        "id": "l_RrrxhMOnSk"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Logistic\")\n",
        "test_model_per_class_by_output(classifier_log, output_type=\"log(K)\", sample_size=1000, n=30)\n",
        "print()\n",
        "\n",
        "print(\"Cross Entropy\")\n",
        "test_model_per_class_by_output(classifier_ce, output_type=\"p\", sample_size=1000, n=30)\n",
        "print()\n",
        "\n",
        "print(\"Exponential\")\n",
        "test_model_per_class_by_output(classifier_exp, output_type=\"log(K)\", sample_size=1000, n=30)\n",
        "print()\n",
        "\n",
        "print(\"Alpha Exponential Log\")\n",
        "test_model_per_class_by_output(classifier_alpha_exp_log, output_type=\"K\", sample_size=1000, n=30)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1pmoYrDgWQL",
        "outputId": "4fbca633-322f-4a34-fc78-e9e6054a1051"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic\n",
            "Model 0 accuracy: 86.40% (432/500)\n",
            "Model 1 accuracy: 69.40% (347/500)\n",
            "\n",
            "Cross Entropy\n",
            "Model 0 accuracy: 89.60% (448/500)\n",
            "Model 1 accuracy: 71.80% (359/500)\n",
            "\n",
            "Exponential\n",
            "Model 0 accuracy: 95.20% (476/500)\n",
            "Model 1 accuracy: 49.80% (249/500)\n",
            "\n",
            "Alpha Exponential Log\n",
            "Model 0 accuracy: 100.00% (500/500)\n",
            "Model 1 accuracy: 0.00% (0/500)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}